{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Каждый день к нам в datalake приходит суточный файл вида:\n",
    "\n",
    "https://www.sba.gov/sites/default/files/data.json\n",
    "\n",
    "Каждый день к нам будет приходить файл не меньшего размера.\n",
    "\n",
    "1. Необходимо спроектировать структуры для хранения этих данных в hdfs: выбрать подходящую структуру и формат для хранения. Изначально следует заложить некоторое партицирование, которое будет оптимальным для потребителей.\n",
    "\n",
    "Потребителями этих данных будут:\n",
    "\n",
    "* дата аналитики: они будут задавать период для дат, где проводить поиск и часть известных параметров (title, contactPoint.fn, bureauCode и т.д.) для идентификации требуемых документов,\n",
    "\n",
    "* батчовое задание, которое будет считать статистики (кол-во экземпляров по accessLevel, bureauCode, programCode и год из поля identifier) по вновь поступившим изданиям и проверять, что в старых данных нет дублей (pk = identifier).\n",
    "\n",
    "2. необходимо также спроектировать логику работы задания. Следует учесть, что работа дата аналитиков может пересекаться с работой задания и не должно приводить к некорректным результатам в работе их запросов.\n",
    "\n",
    "3. необходимо написать sql-запрос для выполнения job-ы из п.2 (расчет каунтов с учетом дедубликации относительно старых срезов). Если для оптимизации выполнения будут применяться какие-то доп-структуры, то прошу привести некоторое описание по логике их создания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`org.apache.spark::spark-hive:2.4.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkFiles\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "// import org.apache.spark.sql.SQLContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.SparkFiles\n",
    "import org.apache.spark.sql._\n",
    "// import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n",
      "Warning: hive-site.xml not found in the classpath, and no Hive conf found via HIVE_CONF_DIR\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@360f5102\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = (\n",
    "    AmmoniteSparkSession\n",
    "    .builder()\n",
    "    .master(\"local[*]\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "import spark.implicits._\n",
    "spark.sparkContext.setLogLevel(\"FATAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.catalog.clearCache()\n",
    "// spark.close()\n",
    "// spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Загрузим источник**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- @context: string (nullable = true)\n",
      " |-- @type: string (nullable = true)\n",
      " |-- conformsTo: string (nullable = true)\n",
      " |-- dataset: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- @type: string (nullable = true)\n",
      " |    |    |-- accessLevel: string (nullable = true)\n",
      " |    |    |-- accrualPeriodicity: string (nullable = true)\n",
      " |    |    |-- bureauCode: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- contactPoint: struct (nullable = true)\n",
      " |    |    |    |-- @type: string (nullable = true)\n",
      " |    |    |    |-- fn: string (nullable = true)\n",
      " |    |    |    |-- hasEmail: string (nullable = true)\n",
      " |    |    |-- dataQuality: boolean (nullable = true)\n",
      " |    |    |-- describedBy: string (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- distribution: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- @type: string (nullable = true)\n",
      " |    |    |    |    |-- accessURL: string (nullable = true)\n",
      " |    |    |    |    |-- describedBy: string (nullable = true)\n",
      " |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |-- downloadURL: string (nullable = true)\n",
      " |    |    |    |    |-- format: string (nullable = true)\n",
      " |    |    |    |    |-- mediaType: string (nullable = true)\n",
      " |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- identifier: string (nullable = true)\n",
      " |    |    |-- isPartOf: string (nullable = true)\n",
      " |    |    |-- issued: string (nullable = true)\n",
      " |    |    |-- keyword: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- landingPage: string (nullable = true)\n",
      " |    |    |-- language: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- license: string (nullable = true)\n",
      " |    |    |-- modified: string (nullable = true)\n",
      " |    |    |-- programCode: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- publisher: struct (nullable = true)\n",
      " |    |    |    |-- @type: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- rights: string (nullable = true)\n",
      " |    |    |-- theme: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |-- describedBy: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [@context: string, @type: string ... 3 more fields]\n",
       "\u001b[36mdataset_meta\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"{\\\"@context\\\":\\\"https://project-open-data.cio.gov/v1.1/schema/catalog.jsonld\\\",\\\"@type\\\":\\\"dcat:Catalog\\\",\\\"conformsTo\\\":\\\"https://project-open-data.cio.gov/v1.1/schema\\\",\\\"describedBy\\\":\\\"https://project-open-data.cio.gov/v1.1/schema/catalog.json\\\"}\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.addFile(\"https://www.sba.gov/sites/default/files/data.json\")\n",
    "\n",
    "val data = spark.read.json(\"file://\"+SparkFiles.get(\"data.json\"))\n",
    "val dataset_meta = data.select(\"@context\",\"@type\",\"conformsTo\",\"describedBy\").toJSON.collect()(0)\n",
    "\n",
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Сохраним источник**\n",
    "<br>**в реальной ситуации будем партиционировать по дате загрузки**\n",
    "<br>**Или по дате модификации записи ```modified```, но это усложнит процесс апдейта данных**\n",
    "<br>**На данный момент сложно сразу сказать как будет оптимальнее**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data\n",
    "    //.withColumn(\"load_date\", expr(\"CURRENT_DATE\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    //.partitionBy(\"load_date\")\n",
    "    .json(\"hdfs/source_daily_batch_json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Будем считать, что схема прогруженной части данных обладает полным набором полей**\n",
    "<br>**Сохраним для дальнейшего использования**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import java.io._\n",
    "\n",
    "// val schemaDDL1 = data.schema.toDDL\n",
    "// // spark.sparkContext.parallelize(Array(schemaDDL)).toDF.toJSON.coalesce(1).write.mode(\"overwrite\").json(\"hdfs/meta_schemaDDL\")\n",
    "// val writerDDL1 = new PrintWriter(new File(\"schemaDDL1.txt\"))\n",
    "// writerDDL1.write(schemaDDL1)\n",
    "// writerDDL1.close()\n",
    "\n",
    "// import java.io._\n",
    "// val schemaDDL2 = data.select(explode($\"dataset\") as \"dataset\").select(\"dataset.*\").schema.toDDL\n",
    "// val writerDDL2 = new PrintWriter(new File(\"schemaDDL2.txt\"))\n",
    "// writerDDL2.write(schemaDDL2)\n",
    "// writerDDL2.close()\n",
    "\n",
    "// val schemaJSON = data.schema.json\n",
    "// // spark.sparkContext.parallelize(Array(schemaJSON)).toDF.toJSON.coalesce(1).write.mode(\"overwrite\").json(\"hdfs/meta_schemaJSON\")\n",
    "// val writerJSON = new PrintWriter(new File(\"schemaJSON.json\"))\n",
    "// writerJSON.write(schemaJSON)\n",
    "// writerJSON.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mschemaDDL1str\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"`@context` STRING,`@type` STRING,`conformsTo` STRING,`dataset` ARRAY<STRUCT<`@type`: STRING, `accessLevel`: STRING, `accrualPeriodicity`: STRING, `bureauCode`: ARRAY<STRING>, `contactPoint`: STRUCT<`@type`: STRING, `fn`: STRING, `hasEmail`: STRING>, `dataQuality`: BOOLEAN, `describedBy`: STRING, `description`: STRING, `distribution`: ARRAY<STRUCT<`@type`: STRING, `accessURL`: STRING, `describedBy`: STRING, `description`: STRING, `downloadURL`: STRING, `format`: STRING, `mediaType`: STRING, `title`: STRING>>, `identifier`: STRING, `isPartOf`: STRING, `issued`: STRING, `keyword`: ARRAY<STRING>, `landingPage`: STRING, `language`: ARRAY<STRING>, `license`: STRING, `modified`: STRING, `programCode`: ARRAY<STRING>, `publisher`: STRUCT<`@type`: STRING, `name`: STRING>, `rights`: STRING, `theme`: ARRAY<STRING>, `title`: STRING>>,`describedBy` STRING\"\u001b[39m\n",
       "\u001b[36mschemaDDL2str\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"`@type` STRING,`accessLevel` STRING,`accrualPeriodicity` STRING,`bureauCode` ARRAY<STRING>,`contactPoint` STRUCT<`@type`: STRING, `fn`: STRING, `hasEmail`: STRING>,`dataQuality` BOOLEAN,`describedBy` STRING,`description` STRING,`distribution` ARRAY<STRUCT<`@type`: STRING, `accessURL`: STRING, `describedBy`: STRING, `description`: STRING, `downloadURL`: STRING, `format`: STRING, `mediaType`: STRING, `title`: STRING>>,`identifier` STRING,`isPartOf` STRING,`issued` STRING,`keyword` ARRAY<STRING>,`landingPage` STRING,`language` ARRAY<STRING>,`license` STRING,`modified` STRING,`programCode` ARRAY<STRING>,`publisher` STRUCT<`@type`: STRING, `name`: STRING>,`rights` STRING,`theme` ARRAY<STRING>,`title` STRING\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schemaDDL1str = \"`@context` STRING,`@type` STRING,`conformsTo` STRING,`dataset` ARRAY<STRUCT<`@type`: STRING, `accessLevel`: STRING, `accrualPeriodicity`: STRING, `bureauCode`: ARRAY<STRING>, `contactPoint`: STRUCT<`@type`: STRING, `fn`: STRING, `hasEmail`: STRING>, `dataQuality`: BOOLEAN, `describedBy`: STRING, `description`: STRING, `distribution`: ARRAY<STRUCT<`@type`: STRING, `accessURL`: STRING, `describedBy`: STRING, `description`: STRING, `downloadURL`: STRING, `format`: STRING, `mediaType`: STRING, `title`: STRING>>, `identifier`: STRING, `isPartOf`: STRING, `issued`: STRING, `keyword`: ARRAY<STRING>, `landingPage`: STRING, `language`: ARRAY<STRING>, `license`: STRING, `modified`: STRING, `programCode`: ARRAY<STRING>, `publisher`: STRUCT<`@type`: STRING, `name`: STRING>, `rights`: STRING, `theme`: ARRAY<STRING>, `title`: STRING>>,`describedBy` STRING\"\n",
    "val schemaDDL2str = \"`@type` STRING,`accessLevel` STRING,`accrualPeriodicity` STRING,`bureauCode` ARRAY<STRING>,`contactPoint` STRUCT<`@type`: STRING, `fn`: STRING, `hasEmail`: STRING>,`dataQuality` BOOLEAN,`describedBy` STRING,`description` STRING,`distribution` ARRAY<STRUCT<`@type`: STRING, `accessURL`: STRING, `describedBy`: STRING, `description`: STRING, `downloadURL`: STRING, `format`: STRING, `mediaType`: STRING, `title`: STRING>>,`identifier` STRING,`isPartOf` STRING,`issued` STRING,`keyword` ARRAY<STRING>,`landingPage` STRING,`language` ARRAY<STRING>,`license` STRING,`modified` STRING,`programCode` ARRAY<STRING>,`publisher` STRUCT<`@type`: STRING, `name`: STRING>,`rights` STRING,`theme` ARRAY<STRING>,`title` STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mddlSchema1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"@context\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"conformsTo\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    \u001b[32m\"dataset\"\u001b[39m,\n",
       "    \u001b[33mArrayType\u001b[39m(\n",
       "      \u001b[33mStructType\u001b[39m(\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"accessLevel\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"accrualPeriodicity\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"bureauCode\"\u001b[39m, \u001b[33mArrayType\u001b[39m(StringType, true), true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\n",
       "          \u001b[32m\"contactPoint\"\u001b[39m,\n",
       "          \u001b[33mStructType\u001b[39m(\n",
       "            \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "            \u001b[33mStructField\u001b[39m(\u001b[32m\"fn\"\u001b[39m, StringType, true, {}),\n",
       "            \u001b[33mStructField\u001b[39m(\u001b[32m\"hasEmail\"\u001b[39m, StringType, true, {})\n",
       "          ),\n",
       "          true,\n",
       "          {}\n",
       "        ),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"dataQuality\"\u001b[39m, BooleanType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"describedBy\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"description\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\n",
       "          \u001b[32m\"distribution\"\u001b[39m,\n",
       "          \u001b[33mArrayType\u001b[39m(\n",
       "            \u001b[33mStructType\u001b[39m(\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"accessURL\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"describedBy\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"description\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"downloadURL\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"format\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"mediaType\"\u001b[39m, StringType, true, {}),\n",
       "              \u001b[33mStructField\u001b[39m(\u001b[32m\"title\"\u001b[39m, StringType, true, {})\n",
       "            ),\n",
       "            true\n",
       "...\n",
       "\u001b[36mddlSchema2\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"accessLevel\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"accrualPeriodicity\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"bureauCode\"\u001b[39m, \u001b[33mArrayType\u001b[39m(StringType, true), true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    \u001b[32m\"contactPoint\"\u001b[39m,\n",
       "    \u001b[33mStructType\u001b[39m(\n",
       "      \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "      \u001b[33mStructField\u001b[39m(\u001b[32m\"fn\"\u001b[39m, StringType, true, {}),\n",
       "      \u001b[33mStructField\u001b[39m(\u001b[32m\"hasEmail\"\u001b[39m, StringType, true, {})\n",
       "    ),\n",
       "    true,\n",
       "    {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"dataQuality\"\u001b[39m, BooleanType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"describedBy\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"description\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    \u001b[32m\"distribution\"\u001b[39m,\n",
       "    \u001b[33mArrayType\u001b[39m(\n",
       "      \u001b[33mStructType\u001b[39m(\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"@type\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"accessURL\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"describedBy\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"description\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"downloadURL\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"format\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"mediaType\"\u001b[39m, StringType, true, {}),\n",
       "        \u001b[33mStructField\u001b[39m(\u001b[32m\"title\"\u001b[39m, StringType, true, {})\n",
       "      ),\n",
       "      true\n",
       "    ),\n",
       "    true,\n",
       "    {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"identifier\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"isPartOf\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"issued\"\u001b[39m, StringType, true, {}),\n",
       "..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ddlSchema1 = StructType.fromDDL(schemaDDL1str)\n",
    "// ddlSchema1.printTreeString()\n",
    "val ddlSchema2 = StructType.fromDDL(schemaDDL2str)\n",
    "// ddlSchema2.printTreeString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Если дата-аналитики не против работать с комплексными структурами - сохраним для них часть данных в Hive таблицу, партицированную по дате загрузки**\n",
    "<br>**Для батча - сохраним часть данных в parquet**\n",
    "<br>**Что делать с дублямя-как апдейтить и как учесть дату модификации - не решил**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: string, value: string]\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: string, value: string]\n",
       "\u001b[36mres7_2\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres7_3\u001b[39m: \u001b[32mDataFrame\u001b[39m = []\n",
       "\u001b[36mres7_4\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" SET hive.exec.dynamic.partition=true \"\"\")\n",
    "spark.sql(\"\"\" SET hive.exec.dynamic.partition.mode=nonstrict \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS DATASET_DAILY_FULL\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE DATASET_DAILY_FULL (\n",
    "    `type` STRING,\n",
    "    `accessLevel` STRING,\n",
    "    `accrualPeriodicity` STRING,\n",
    "    `bureauCode` ARRAY<STRING>,\n",
    "    `contactPoint` STRUCT<`type`: STRING, `fn`: STRING, `hasEmail`: STRING>,\n",
    "    `dataQuality` BOOLEAN,\n",
    "    `describedBy` STRING,\n",
    "    `description` STRING,\n",
    "    `distribution` ARRAY<STRUCT<`type`: STRING, `accessURL`: STRING, `describedBy`: STRING, `description`: STRING, `downloadURL`: STRING, `format`: STRING, `mediaType`: STRING, `title`: STRING>>,\n",
    "    `identifier` STRING,\n",
    "    `isPartOf` STRING,\n",
    "    `issued` STRING,\n",
    "    `keyword` ARRAY<STRING>,\n",
    "    `landingPage` STRING,\n",
    "    `language` ARRAY<STRING>,\n",
    "    `license` STRING,\n",
    "    `modified` STRING,\n",
    "    `programCode` ARRAY<STRING>,\n",
    "    `publisher` STRUCT<`type`: STRING, `name`: STRING>,\n",
    "    `rights`  STRING,\n",
    "    `theme` ARRAY<STRING>,\n",
    "    `title` STRING\n",
    ")\n",
    "PARTITIONED BY (load_date date)\n",
    "STORED AS PARQUET \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS DATASET_DAILY\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Используя ранее сохранные данные ```data.json``` и схему ```ddlSchema1``` сэмулируем запись в табилицу и hdfs через частичное сэмплирование, итерируясь по ```dateRange```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdateRange\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"2020-01-01\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-02\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-03\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-04\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-05\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-06\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-07\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-08\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-09\"\u001b[39m,\n",
       "  \u001b[32m\"2020-01-10\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateRange: Array[String] = spark.sql(\"SELECT explode(sequence(to_date('2020-01-01'), to_date('2020-01-10'), interval 1 day))\").collect.map(_(0).toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36muploadData\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "\n",
    "def uploadData(spark: org.apache.spark.sql.SparkSession, current_date: String) = {\n",
    "    val records = (\n",
    "        spark.read.schema(ddlSchema1).json(\"hdfs/source_daily_batch_json\")\n",
    "        .select(explode($\"dataset\") as \"dataset\")\n",
    "        .select(\"dataset.*\")\n",
    "        .sample(false, 0.66)\n",
    "        .withColumn(\"load_date\", expr(s\"cast('${current_date}' as date)\"))\n",
    "        )\n",
    "    val cnt1 = records.select(\"identifier\").distinct.count\n",
    "    \n",
    "    // данные для батч-джобы\n",
    "    records.write.mode(\"overwrite\").partitionBy(\"load_date\").parquet(\"hdfs/source_daily_batch_parquet\")\n",
    "    \n",
    "    // данные для аналитиков\n",
    "    records.createOrReplaceTempView(\"RECORDS\")\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT OVERWRITE TABLE \n",
    "            DATASET_DAILY_FULL PARTITION(load_date) \n",
    "        SELECT \n",
    "            *\n",
    "        FROM\n",
    "            RECORDS\"\"\")\n",
    "    val cnt2 = spark.sql(s\"\"\" SELECT DISTINCT identifier FROM DATASET_DAILY_FULL WHERE load_date='${current_date}' \"\"\").count()\n",
    "    \n",
    "    //check \n",
    "    println(\"(uniq. identifier) \"+current_date+\": \"+cnt1+\", \"+cnt2+\" loaded from source\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(uniq. identifier) 2020-01-01: 621, 621 loaded from source\n"
     ]
    }
   ],
   "source": [
    "uploadData(spark, dateRange(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msparkJob\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparkJob(spark: org.apache.spark.sql.SparkSession, schemaRec: types.StructType, current_date: String) = {\n",
    "    \n",
    "    // кол-во экземпляров по accessLevel, bureauCode, programCode и год из поля identifier\n",
    "    // println(current_date)\n",
    "    \n",
    "    val records_full = spark.read.schema(ddlSchema2).parquet(\"hdfs/source_daily_batch_parquet/\")\n",
    "    val records_prev = records_full.filter(s\" load_date < cast('${current_date}' as date) \").select($\"identifier\" as \"identifier_prev\").distinct\n",
    "    val records_curr = records_full.filter(s\" load_date = cast('${current_date}' as date) \")\n",
    "    val records = records_prev.join(records_curr, expr(\"identifier_prev = identifier\"), \"outer\")\n",
    "    \n",
    "    val dublrec = records.filter(\"identifier_prev is not null and identifier is not null\").count\n",
    "    println(s\"REPORT: '${current_date}' \\n\\nfound dubl: \" + dublrec+\"\\n\")\n",
    "    \n",
    "    val results = records.filter(\"identifier_prev IS NULL\").drop(\"identifier_prev\")\n",
    "    \n",
    "    // TODO: replace DISTINCT by LAST(modified) ???\n",
    "    // TODO: add check for empty batch\n",
    "    \n",
    "    val accessLevel = results.select($\"identifier\", $\"accessLevel\").distinct.groupBy(\"accessLevel\").agg(count($\"identifier\")).show()\n",
    "    val bureauCode  = results.select($\"identifier\", explode($\"bureauCode\") as \"bureauCode\").distinct.groupBy(\"bureauCode\").agg(count($\"identifier\")).show()\n",
    "    val programCode  = results.select($\"identifier\", explode($\"programCode\") as \"programCode\").distinct.groupBy(\"programCode\").agg(count($\"identifier\")).show()\n",
    "    val identifierYear = (\n",
    "        results\n",
    "        .select($\"identifier\", regexp_extract($\"identifier\", \"(\\\\d{4})(-\\\\d{2}-\\\\d{3})\", 1) as \"identifierYear\")\n",
    "        .distinct.groupBy(\"identifierYear\").agg(count($\"identifier\")).orderBy(\"identifierYear\").show())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> ```RUN EXPERIMENT```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(uniq. identifier) 2020-01-02: 596, 596 loaded from source\n",
      "REPORT: '2020-01-02' \n",
      "\n",
      "found dubl: 393\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|              196|\n",
      "| non-public|                7|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|              203|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:017|                6|\n",
      "|    028:000|                8|\n",
      "|    028:001|               37|\n",
      "|    028:031|              150|\n",
      "|    028:002|                2|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2013|                1|\n",
      "|          2015|               44|\n",
      "|          2016|               58|\n",
      "|          2017|               18|\n",
      "|          2018|               33|\n",
      "|          2019|               38|\n",
      "|          2020|               11|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-03: 604, 604 loaded from source\n",
      "REPORT: '2020-01-03' \n",
      "\n",
      "found dubl: 545\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|               57|\n",
      "| non-public|                2|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|               59|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:017|                2|\n",
      "|    028:000|                4|\n",
      "|    028:001|                8|\n",
      "|    028:031|               45|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2014|                1|\n",
      "|          2015|               16|\n",
      "|          2016|               14|\n",
      "|          2017|                6|\n",
      "|          2018|                8|\n",
      "|          2019|               13|\n",
      "|          2020|                1|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-04: 576, 576 loaded from source\n",
      "REPORT: '2020-01-04' \n",
      "\n",
      "found dubl: 554\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|               22|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|               22|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:001|                3|\n",
      "|    028:031|               18|\n",
      "|    028:002|                1|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2015|                6|\n",
      "|          2016|                6|\n",
      "|          2017|                3|\n",
      "|          2018|                3|\n",
      "|          2019|                2|\n",
      "|          2020|                2|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-05: 577, 577 loaded from source\n",
      "REPORT: '2020-01-05' \n",
      "\n",
      "found dubl: 565\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|               12|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|               12|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:001|                1|\n",
      "|    028:031|               11|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2015|                5|\n",
      "|          2016|                3|\n",
      "|          2017|                1|\n",
      "|          2018|                1|\n",
      "|          2019|                1|\n",
      "|          2020|                1|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-06: 618, 618 loaded from source\n",
      "REPORT: '2020-01-06' \n",
      "\n",
      "found dubl: 615\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|                3|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|                3|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:001|                1|\n",
      "|    028:031|                2|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2015|                1|\n",
      "|          2019|                2|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-07: 604, 604 loaded from source\n",
      "REPORT: '2020-01-07' \n",
      "\n",
      "found dubl: 603\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|                1|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|                1|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:001|                1|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2018|                1|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-08: 606, 606 loaded from source\n",
      "REPORT: '2020-01-08' \n",
      "\n",
      "found dubl: 605\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|     public|                1|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "|    028:00|                1|\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "|    028:031|                1|\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "|          2016|                1|\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-09: 617, 617 loaded from source\n",
      "REPORT: '2020-01-09' \n",
      "\n",
      "found dubl: 617\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "+--------------+-----------------+\n",
      "\n",
      "(uniq. identifier) 2020-01-10: 624, 624 loaded from source\n",
      "REPORT: '2020-01-10' \n",
      "\n",
      "found dubl: 624\n",
      "\n",
      "+-----------+-----------------+\n",
      "|accessLevel|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "+-----------+-----------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|bureauCode|count(identifier)|\n",
      "+----------+-----------------+\n",
      "+----------+-----------------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "|programCode|count(identifier)|\n",
      "+-----------+-----------------+\n",
      "+-----------+-----------------+\n",
      "\n",
      "+--------------+-----------------+\n",
      "|identifierYear|count(identifier)|\n",
      "+--------------+-----------------+\n",
      "+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (current_date <- dateRange.slice(1, dateRange.length)) {\n",
    "    \n",
    "    uploadData(spark, current_date)\n",
    "    sparkJob(spark, ddlSchema2, current_date)\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+------------------+-----------+\n",
      "|database|         tableName|isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "| default|dataset_daily_full|      false|\n",
      "|        |           records|       true|\n",
      "+--------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW SCHEMAS\"\"\").show()\n",
    "spark.sql(\"\"\"SHOW TABLES IN DEFAULT\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "| load_date|          identifier|\n",
      "+----------+--------------------+\n",
      "|2020-01-07|SBA-OCIO-2015-08-002|\n",
      "|2020-01-07|SBA-OCIO-2015-08-001|\n",
      "|2020-01-07| SBA-OCA-2017-11-001|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT load_date, identifier FROM dataset_daily_full\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "| load_date|cnt|uniq|\n",
      "+----------+---+----+\n",
      "|2020-01-01|621| 621|\n",
      "|2020-01-02|596| 596|\n",
      "|2020-01-03|604| 604|\n",
      "|2020-01-04|576| 576|\n",
      "|2020-01-05|577| 577|\n",
      "|2020-01-06|618| 618|\n",
      "|2020-01-07|604| 604|\n",
      "|2020-01-08|606| 606|\n",
      "|2020-01-09|617| 617|\n",
      "|2020-01-10|624| 624|\n",
      "+----------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT load_date, count(identifier) as cnt, count(distinct identifier) uniq FROM dataset_daily_full group by load_date order by load_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "| cnt|uniq|\n",
      "+----+----+\n",
      "|6043| 922|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(identifier) as cnt, count(distinct identifier) uniq FROM dataset_daily_full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**Если аналитика на уровне Спарка - не вариант, а комплексные типы - вызывут сложности, то можно вынести данные поля в отдельны сущности**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrecords\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [@type: string, accessLevel: string ... 20 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val records = (\n",
    "    spark.read.schema(ddlSchema1).json(\"hdfs/source_daily_batch_json\")\n",
    "    .select(explode($\"dataset\") as \"dataset\")\n",
    "    .select(\"dataset.*\")\n",
    "    .sample(false, 0.3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE EXAMPLE: BUREAUCODE\n",
      "+--------------------+----------+----------+\n",
      "|          identifier|  modified|bureauCode|\n",
      "+--------------------+----------+----------+\n",
      "|SBA-OCIO-2015-08-002|2019-11-30|    028:00|\n",
      "|SBA-OCIO-2015-08-001|2020-06-12|    028:00|\n",
      "| SBA-OCA-2017-11-004|2017-06-30|    028:00|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "TABLE EXAMPLE: CONTACTPOINT\n",
      "+--------------------+----------+-------------+-------------+--------------------+\n",
      "|          identifier|  modified|        @type|           fn|            hasEmail|\n",
      "+--------------------+----------+-------------+-------------+--------------------+\n",
      "|SBA-OCIO-2015-08-002|2019-11-30|vcard:Contact| Melvin Brown|mailto:melvin.bro...|\n",
      "|SBA-OCIO-2015-08-001|2020-06-12|vcard:Contact| Melvin Brown|mailto:melvin.bro...|\n",
      "| SBA-OCA-2017-11-004|2017-06-30|vcard:Contact|Ronald Whalen|mailto:Ronald.Wha...|\n",
      "+--------------------+----------+-------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "TABLE EXAMPLE: DISTRIBUTION\n",
      "+--------------------+-----------------+--------------------+-----------+-----------+--------------------+------+---------------+--------------------+\n",
      "|          identifier|            @type|           accessURL|describedBy|description|         downloadURL|format|      mediaType|               title|\n",
      "+--------------------+-----------------+--------------------+-----------+-----------+--------------------+------+---------------+--------------------+\n",
      "|SBA-OCIO-2015-08-002|dcat:Distribution|https://www.sba.g...|       null|       null|                null|  null|           null|SBA IT Policy Arc...|\n",
      "|SBA-OCIO-2015-08-002|dcat:Distribution|                null|       null|       null|https://inventory...|  null|application/zip|SBA IT Policy Arc...|\n",
      "|SBA-OCIO-2015-08-001|dcat:Distribution|https://www.sba.g...|       null|       null|                null|  null|           null|Bureau IT Leaders...|\n",
      "+--------------------+-----------------+--------------------+-----------+-----------+--------------------+------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "TABLE EXAMPLE: KEYWORD\n",
      "+--------------------+----------+--------------------+\n",
      "|          identifier|  modified|             keyword|\n",
      "+--------------------+----------+--------------------+\n",
      "|SBA-OCIO-2015-08-002|2019-11-30|Agency IT Policy ...|\n",
      "|SBA-OCIO-2015-08-002|2019-11-30|SBA IT Policy Arc...|\n",
      "|SBA-OCIO-2015-08-001|2020-06-12|Bureau IT Leaders...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "TABLE EXAMPLE: PROGRAMCODE\n",
      "+--------------------+----------+-----------+\n",
      "|          identifier|  modified|programCode|\n",
      "+--------------------+----------+-----------+\n",
      "|SBA-OCIO-2015-08-002|2019-11-30|    028:000|\n",
      "|SBA-OCIO-2015-08-001|2020-06-12|    028:000|\n",
      "| SBA-OCA-2017-11-004|2017-06-30|    028:001|\n",
      "+--------------------+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "TABLE EXAMPLE: PUBLISHER\n",
      "+--------------------+----------+----------------+--------------------+\n",
      "|          identifier|  modified|           @type|                name|\n",
      "+--------------------+----------+----------------+--------------------+\n",
      "|SBA-OCIO-2015-08-002|2019-11-30|org:Organization|U.S. Small Busine...|\n",
      "|SBA-OCIO-2015-08-001|2020-06-12|org:Organization|U.S. Small Busine...|\n",
      "| SBA-OCA-2017-11-004|2017-06-30|org:Organization|U.S. Small Busine...|\n",
      "+--------------------+----------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "TABLE EXAMPLE: THEME\n",
      "+--------------------+----------+---------------+\n",
      "|          identifier|  modified|          theme|\n",
      "+--------------------+----------+---------------+\n",
      "|SBA-GCDB-2013-11-001|2020-04-13|    search tool|\n",
      "|SBA-GCDB-2013-11-001|2020-04-13|user controlled|\n",
      "|SBA-GCDB-2013-11-001|2020-04-13|        dynamic|\n",
      "+--------------------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"TABLE EXAMPLE: \"+\"bureauCode\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", explode($\"bureauCode\") as \"bureauCode\").show(3)\n",
    "\n",
    "println(\"TABLE EXAMPLE: \"+\"contactPoint\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", $\"contactPoint.*\").show(3)\n",
    "\n",
    "println(\"TABLE EXAMPLE: \"+\"distribution\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", explode($\"distribution\") as \"distribution\").select($\"identifier\", $\"distribution.*\").show(3)\n",
    "\n",
    "println(\"TABLE EXAMPLE: \"+\"keyword\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", explode($\"keyword\") as \"keyword\").show(3)\n",
    "\n",
    "println(\"TABLE EXAMPLE: \"+\"programCode\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", explode($\"programCode\") as \"programCode\").show(3)\n",
    "\n",
    "println(\"TABLE EXAMPLE: \"+\"publisher\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", $\"publisher.*\").show(3)\n",
    "\n",
    "println(\"TABLE EXAMPLE: \"+\"theme\".toUpperCase)\n",
    "records.select($\"identifier\", $\"modified\", explode($\"theme\") as \"theme\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.close()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> **Notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val cols = Array(\"type\",\"accessURL\",\"describedBy\", \"description\", \"downloadURL\", \"format\",\"mediaType\", \"title\").map(x=>count(col(x)))\n",
    "// recordReaded.select($\"identifier\", explode($\"distribution\") as \"distribution\").select($\"identifier\", $\"distribution.*\").groupBy().agg(cols.head, cols.tail: _*).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val cols = Array(\"@type\",\"accessURL\",\"describedBy\", \"description\", \"downloadURL\", \"format\",\"mediaType\", \"title\").map(x=>count(col(x)))\n",
    "// records.select($\"identifier\", explode($\"distribution\") as \"distribution\").select($\"identifier\", $\"distribution.*\").groupBy().agg(cols.head, cols.tail: _*).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val identifierDate = regexp_extract($\"identifier\", \"(\\\\d{4}-\\\\d{2}-\\\\d{3})\", 0)\n",
    "// records.select(\"identifier\").withColumn(\"identifierDate\", identifierDate).write.mode(\"overwrite\").partitionBy(\"identifierDate\").parquet(\"data/stg_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// val identifierYear = regexp_extract($\"identifier\", \"(\\\\d{4})(-\\\\d{2}-\\\\d{3})\", 1)\n",
    "// records.select(\"identifier\").withColumn(\"identifierYear\", identifierYear).write.mode(\"overwrite\").partitionBy(\"identifierYear\").parquet(\"data/stg_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
